# -*- coding: utf-8 -*-
"""
pytorch_fundamentals.ipynb
Automatically generated by Colaboratory.

Author: Uygar Kaya
"""

import torch
import pandas as pd 
import numpy as np

from matplotlib import pyplot as plt

"""
## Introduction to Tensors
# Creating Tensors
"""

# Scalar
scalar = torch.tensor(7) # Create a Scalar
scalar.item() # Get the Python int from PyTorch Scalar

# Vector
vector = torch.tensor([5, 6, 7])
vector

# Matrix
matrix = torch.tensor([[7, 7],
                              [4, 3]])
matrix

# Tensor
tensor = torch.tensor([[[1, 2, 3], 
                        [4, 5, 6],
                        [7, 8, 9]]])

tensor

"""## Random Tensor"""

# Create a Random Tensor
random_tensor = torch.rand(3, 4)
random_tensor

# Create a random tensor with similar shape to an image
random_image_tensor = torch.rand(250, 250, 3) # height, widgth, colour channel -> (R, G, B)
random_image_tensor.shape, random_image_tensor.ndim

"""## Zeros and Ones"""

# Create a tensor of all zeros
zeros = torch.zeros(size=(3, 4))
zeros

# Create a tensor of all Ones
ones = torch.ones(size=(3, 4))
ones

"""## Creating a Range of Tensors and Tensors-Like"""

# Use a torch.range()

zero_to_nine = torch.arange(start=0, end=10)
range_with_step = torch.arange(0, 1000, 50)

range_with_step

# Creating a tensors like
ten_zeros = torch.zeros_like(input=zero_to_nine)
ten_zeros

""" 
## Getting Information from Tensors

 1. Tensors Datatype -> to do get datatype from a tensor, can use `tensor.dtype`
 2. Tensors Shape -> to do get shape from tensor, can use `tensor.shape`
 3. Tensors Device -> to do get device info from tensor, can use `tensor.device`
"""

# Create a tensor
example_tensor = torch.rand(size=(5, 6), device='cpu')
example_tensor

# Find out details about some tensor
print(f'Datatype of tensor: {example_tensor.dtype}')
print(f'Shape of tensor: {example_tensor.shape}')
print(f'Device info of tensor: {example_tensor.device}')

"""## Manipulating Tensors (tensor operations)

Tensor Operations include:
  * Addition
  * Subtraction
  * Multiplication
  * Division
  * Matrix Multiplication
"""

# Tensor Operation - Addition 

# Create a random tensor
tensor = torch.rand(3, 4)
print(f'before the addition: {tensor}')
tensor = tensor + 10 # addition the tensor for each element
print(f'after the addition: {tensor}')

# Tensor Operation - Multiplication

# Create a random tensor
tensor = torch.rand(3, 4)
print(f'before the multiplication: {tensor}')
tensor = tensor * 10 # multiply the tensor for each element
print(f'after the multiplication: {tensor}')

# Tensor Operation - Subtruction

# Create a random tensor
tensor = torch.rand(3, 4)
print(f'before the subtraction: {tensor}')
tensor = tensor - 10 # subtract the tensor for each element
print(f'after the subtraction: {tensor}')

# Tensor Operation - Division

# Create a random tensor
tensor = torch.rand(3, 4)
print(f'before the division: {tensor}')
tensor = tensor / 10 # division the tensor for each element
print(f'after the division: {tensor}')

"""
## Matrix Multiplication
  * Element-wise Multiplication
  * Matrix Multiplication - (dot product)

The main two rules for matrix multiplication to remember are:
  1. The ***inner dimensions*** must match:

  * `(3, 2) @ (3, 2)` -> won't work
  * `(2, 3) @ (3, 2)` -> will work
  * `(3, 2) @ (2, 3)` -> will work

  2. The resulting matrix has the shape of the ***outer dimensions***:

  * `(2, 3) @ (3, 2) -> (2, 2)`
  * `(3, 2) @ (2, 3) -> (3, 3)`

Note: `torch.matmul()`, `torch.mm()`, `@` in python is the symbol and functions for matrix multiplication.
"""

# Element-wise Operation

tensor = torch.tensor([1, 2, 3])
print(tensor, '*', tensor)
print(f'Equals: {tensor * tensor}')

# Matrix Multiplication

first_tensor = torch.rand(2, 3)
second_tensor = torch.rand(3, 2)
matrix_mul_result = torch.matmul(first_tensor, second_tensor) # torch.matmul(), torch.mm(), @ -> Matrix Multiplication
print(f'Result: {matrix_mul_result}')

## Finding the min, max, mean, sum, etc - (tensor aggregation)
# Create a tensor

tensor = torch.arange(start=0, end=100, step=10)

# Minimum value of tensor -> torch.min(tensor) or tensor.min()
print(f'Min: {torch.min(tensor)}')

# Maximum value of tensor -> torch.max(tensor) or tensor.max()
print(f'Max: {torch.max(tensor)}')

# Mean value of tensor -> but before applying, you should convert it to float32 dtype
# torch.mean(tensor.type(torch.float32)), tensor.type(torch.float32).mean()
print(f'Mean: {torch.mean(tensor.type(torch.float32))}')

# Sum of tensor -> torch.sum(tensor) or tensor.sum()
print(f'Sum: {torch.sum(tensor)}')

# Find the position in tensor that has the minimum value with argmin()
print(f'Index of min value: {tensor.argmin()}')

# Find the position in tensor that has the maximum value with argmax()
print(f'Index of max value: {tensor.argmax()}')

"""
## Reshaping, stacking, squeezing, and unsqueezing

* Reshaping - reshapes an input tensor to a defined shape
* View - return a view of an input tensor of certain shape but keep the same memory as the original tensor
* Stacking - combine multiple tensors on top of each other (vstack) or side by side (hstack)
* Squeeze - removes all `1` dimensions from tensor
* Unsqueeze - add a `1` dimension to a target tensor
* Permute - Return a view of the input with dimensions permuted (swapped) in a certain way
"""

# Let's create a tensor
tensor = torch .arange(1., 10.)
print(f'tensor: {tensor}')
print(f'shape of tensor: {tensor.shape}')

# Reshape - add a extra dimension

tensor_reshape = tensor.reshape(1, 9)
print(f'reshaped tensor: {tensor_reshape}')
print(f'reshaped tensor shape: {tensor_reshape.shape}')

# View - change the view
# It's similar to reshape, but view keep the same memory as the original tensor
# Changes to view change the tensor 

view = tensor.view(1, 9)
print(f'view: {view}')
print(f'view of tensor: {view.shape}')

# Stack - stack tensors on top of each other
# Change the `dim` and see what's happening

tensor_stacked = torch.stack([tensor, tensor, tensor], dim=0)
tensor_stacked

# torch.squeeze() - removes all single dimensions from a target tensor

print(f'before applying squeeze - tensor: {tensor_reshape}')
print(f'before applying squeeze - shape: {tensor_reshape.shape}')
squeeze = torch.squeeze(tensor_reshape)
print(f'after applying squeeze - tensor: {squeeze}')
print(f'after applying squeeze - shape: {squeeze.shape}')

# torch.unsqueeze() - add a single dimensions to a target tensor

print(f'before applying unsqueeze - tensor: {squeeze}')
print(f'before applying unsqueeze - shape: {squeeze.shape}')
unsqueeze = torch.unsqueeze(squeeze, dim=0)
print(f'after applying unsqueeze - tensor: {unsqueeze}')
print(f'after applying unsqueeze - shape: {unsqueeze.shape}')

# torch.permute() - rearranges the dimensions of a target tensor in a specified order

original_tensor = torch.rand(size=(224, 224, 3)) # [height, width, colour_channels]
permuted_tensor = torch.permute(original_tensor, (2, 0, 1)) # shifts axis 0->1, 1->2, 2->0

print(f'before applying permute - shape: {original_tensor.shape}')
print(f'after applying permute - shape: {permuted_tensor.shape}')

"""
## Indexing (selecting data from tensor)

* Indexing with PyTorch is similar to indexing with NumPy
"""

# Create a tensor
tensor = torch.arange(1, 10).reshape(1, 3, 3)
tensor

# Let's index on our new tensor
tensor[0], tensor[0, 0], tensor[0][0][0]

"""## PyTorch tensor & NumPy"""

import torch
import numpy as np

# NumPy array to PyTorch tensor
# Create a NumPy array
array = np.arange(1., 10.)
tensor = torch.from_numpy(array)

print(f'NumPy Array: {array}')
print(f'PyTorch Tensor: {tensor}')

# PyTorch tensor to NumPy array
# Create a tensor
tensor = torch.ones(7)
array = np.array(tensor)

print(f'PyTorch Tensor: {tensor}')
print(f'NumPy Array: {array}')

"""## Reproducbility (trying to take random out of random)"""

# Create a random tensor

random_tensor_a = torch.rand(3, 4)
random_tensor_b = torch.rand(3, 4)

print(f'A: {random_tensor_a}')
print(f'B: {random_tensor_b}')
print(f'E: {random_tensor_a == random_tensor_b}')

RANDOM_SEED = 42

torch.manual_seed(RANDOM_SEED)
random_tensor_a = torch.rand(3, 4)

torch.manual_seed(RANDOM_SEED)
random_tensor_b = torch.rand(3, 4)

print(f'A: {random_tensor_a}')
print(f'B: {random_tensor_b}')
print(f'E: {random_tensor_a == random_tensor_b}')

"""##  Running tensors and PyTorch objects on the GPUs"""

# Find the GPU information
print(nvidia-smi)

# Check for GPU access with PyTorch
import torch
torch.cuda.is_available()

# Setup device agnostic code
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

# Count number of devices
torch.cuda.device_count()

"""## Putting tensors/models on the GPU"""

# Create a tensor (default on the CPU)
tensor = torch.rand(3, 4)
print(f'tensor device: {tensor.device}')

# Move tensor to GPU (if avaliable)
tensor_on_cuda = tensor.to(device)
print(f'tensor device: {tensor_on_cuda.device}')

"""## Moving tensors/models back to the CPU"""

# If tensor is on GPU, can't transform it to NumPy
tensor_on_cuda.numpy()

# To fix the GPU tensor with NumPy issue, we can first set it to the CPU
tensor_back_on_cpu = tensor_on_cuda.to('cpu').numpy()
tensor_back_on_cpu